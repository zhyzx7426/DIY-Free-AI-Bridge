# 1. 项目介绍
## 不是 Apple Intelligence 等不到，而是自己的模型更高效

Apple Intelligence 又双叒跳票了，iOS 的移动设备（iPhone/iPad）只能用第三方的 AI 应用，虽然三方应用多如牛毛（比如豆包、腾讯元宝），但因为不是系统级应用，使用起来并不能做到无缝且丝滑。  
想要把正在浏览的内容提问给 AI，还需要把内容复制出来或者截图，再打开 App 进行提问。（第三方 App 无法读取屏幕，隐私安全也不可能允许）。  
所以就研究了本项目，通过 API 调用大模型实现像系统级 AI 应用一样，一键拉起，直接读取屏幕内容，在无需退出当前浏览界面的情况下直接进行问答。

## 实现路径

- **iOS原生应用“快捷指令”简单指令调用API + Cloudflare worker进行多模态处理/流式的拼接 + 自己选择的多模态大模型**
- **安全性上**Cloudflare的worker和大模型仅缓存单次提问数据，请求结束即抛，不会对用户的Prompt和大模型的输出进行存储
- **实现接近系统级AI应用的效果**：扫描屏幕内容，接收语音或文字提问，直接给出结果

## 优势

- 大模型自己选（但要注意提供大模型提供方的隐私政策）  
- 简单几步即可在 iOS 设备上体验系统级 AI 应用的便捷（安卓用户请忽略，苦笑）
- 快捷指令的优势：可以在设置中添加快捷路径，像唤起Siri一样进行唤起

---

本项目以扫描屏幕内容场景进行举例，worker 已支持图片 / 音频输入，可根据大模型支持的模态进一步升级。

快捷指令也可拓展更多（文字、图片、音频等）输入场景。
